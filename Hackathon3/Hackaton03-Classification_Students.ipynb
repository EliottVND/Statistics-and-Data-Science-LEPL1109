{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> \n",
    "<b>Hackathon 03 - Classification: Portuguese wine tasting</b> </font> <br><br><br>\n",
    "\n",
    "<font size=5  color=#003366>\n",
    "Prof. D. Hainaut<br>\n",
    "Prof. L. Jacques<br>\n",
    "\n",
    "<br><br>\n",
    "Anne-Sophie Collin   (anne-sophie.collin@uclouvain.be)<br>\n",
    "Guillaume Van Dessel (guillaume.vandessel@uclouvain.be)<br>\n",
    "Loïc Van Hoorebeeck (loic.vanhoorebeeck@uclouvain.be)<br>\n",
    "Jérome Eertmans (jerome.eertmans@uclouvain.be)<br>\n",
    "Sébastien Colla (sebastien.colla@uclouvain.be)<br>\n",
    "Dani Manjah (dani.manjah@uclouvain.be)<br>\n",
    "<div style=\"text-align: right\"> Version 2021</div>\n",
    "\n",
    "<br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>GUIDELINES & DELIVERABLES</b> </font> <br>\n",
    "-  This assignment is due on the <b>4 December 2021 at 23h59</b>.\n",
    "-  Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups,...) must be clearly indicated!</b>\n",
    "-  This notebook (with the \"ipynb\" extension) file, the Python source file (\".py\"), the report (PDF format) and all other files that are necessary to run your code must be delivered on <b>Moodle</b>.\n",
    "- Only the PDF report and the python source file will be graded, both on their content and the quality of the text / figures. <br><br>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>[DELIVERABLE] Summary</b>  <br>\n",
    "After the reading of this document (and playing with the code!), we expect you to provide us with:\n",
    "<ol>\n",
    "   <li> a PDF file (written in LaTeX, see example on Moodle) that answers all the questions below. The report should contain high quality figures with named axes (we recommend saving plots with the <samp>.pdf</samp> extension);\n",
    "   <li> a Python file with your classifier implementation. Please follow the template that is provided and ensure it passes the so-called <i>sanity</i> tests;\n",
    "   <li> this Jupyter Notebook (it will not be read, just checked for plagiarism);\n",
    "   <li> and all other files (not the datasets!) we would need to run your code.\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<font size=5 color=#009999> <b>CONTEXT & OBJECTIVE </b> </font> <br>\n",
    "    \n",
    "After the completion of your data science course, you decided to give your LEPL1109 assistants team an incredible wine to thank them for their support. At the store, you stand still, lost, in front of a massive range of wine selection. The choice is difficult as they have good taste. Thankfully you come with a brilliant idea that gets two birds with one stone. You use your freshly acquired data science skills to train a model recognizing for you a bottle of quality. You already imagine how impressed they will be when you'll, additionally, explain them how you selected and validated your model.\n",
    "\n",
    "<img src=\"Imgs/wine.png\" width = \"400\">\n",
    "\n",
    "### Dataset description\n",
    "Thankfully you lay hands on a dataset from a wine contest that happened in Minho, a northwest region of Portugal. The dataset includes 5 000 white wines from the variety of *Vinho verde*. Each wine owns a quality score, determined by a jury and its chemical characteristics. The latter are a collection 11 features individually defined in the table below.  \n",
    "\n",
    "<img src=\"Imgs/dataset.png\" width = \"600\">\n",
    "\n",
    "### Objectives\n",
    "The project aims to train a binary classifier distinguishing good and bad wines based on the Portuguese wine dataset. Note that the feature *quality* is originally defined as a natural number between 1 and 10. However, we propose, to better address the problem, to class the wines enjoying a score between 1 and 5 as *bad* (labelled 0) and between 6 and 10 as *good* (labelled 1). The subsequent part of the document will guide you through the process. \n",
    "\n",
    "### Notebook structure\n",
    "This notebook is organized into four parts. Each of them assesses one fundamental step to solve our problem and provides one visualization tool to gain some understanding:\n",
    "* PART 1 - PREPROCESSING\n",
    "   - 1.1 - Import the dataset\n",
    "   - 1.2 - Split the dataset\n",
    "    <br><br>\n",
    "* PART 2 - EXPLORATORY DATA ANALYSIS \n",
    "   - 2.1 - Target proportions\n",
    "   - 2.2 - Correlation matrix\n",
    "   - 2.2 - Features selection\n",
    "   - 2.3 - Scaling the data\n",
    "    <br><br>\n",
    "* PART 3 - MODEL SELECTION\n",
    "   - 3.1 - Precision, recall and F1-score\n",
    "   - 3.2 - Model evaluation\n",
    "   - 3.3 - Model selection and parameters tuning\n",
    "   - 3.4 - Precision-Recall curve and thresholding\n",
    "   <br><br>\n",
    "* PART 4 - MODEL TESTING\n",
    "   - 4.1 - Error computation on the test set \n",
    "   \n",
    "We filled this notebook with preliminary (trivial) code. This practice makes possible to run each cell, even the last ones, without throwing warnings. <b>Take advantage of this aspect to divide the work between all team members!</b> <br><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 1 - PREPROCESSING</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 1.1. IMPORT THE DATASET </font> <br>\n",
    "\n",
    "**Import** `winequality-white.csv` using `read_csv` [<sup>1</sup>](#fn1) from pandas and **obtain** a brief description of the data (_size, variables type, missing values, etc._).  \n",
    "\n",
    "\n",
    "\n",
    "<span id=\"fn1\"> [1] N.B : the separator in the csv file is ';' and not the default comma (see the *`sep`* argument).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 1.1]</b> Describe, briefly, your dataset (size, variables type, missing values, etc.).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1 : Import the dataset using pd.read_csv function\n",
    "\n",
    "@pre: filename 'winequality-white.csv', located in the same folder as this jupyter\n",
    "@post: variable `df` containing the dataframe\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 1.2 - SPLIT THE DATASET </font> <br> \n",
    "\n",
    "Data science projects begin by the division the **whole** dataset into a **training** and a **test** set. The subsequent analysis and decisions (i.e. features selection, pre-processing, model selection, etc.) are, then, conducted only on the _training set_ to stay statistically significant during the **testing phase**. The latter will, thus, only be conducted on the _test set_.  \n",
    "\n",
    "We invite you then to **split** [<sup>2</sup>](#fn2) the dataset into a _training_ and a _test_. The proportion of each subset is at **your own discretion**.\n",
    "\n",
    "\n",
    "<span id=\"fn2\"> [2] N.B. Set the seed of your random split with `random_state = 42` to obtain reproducible results.</span>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 1.2]</b> What are the drawbacks (if any) of choosing a small test set (in proportion)? On the contrary, what are the consequences (if any) of a relatively large testing set (in proportion)? <br>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2 : SPLIT THE DATASET    \n",
    "\n",
    "@pre:  'df' a pandas frame with the entire dataset\n",
    "@post: 2 pandas frames with the train and test sets\n",
    "\"\"\"\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "### split in train and test set\n",
    "# Change this!\n",
    "data_train = df\n",
    "data_test = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><font size=7 color=#009999> <b>PART 2 - EXPLORATORY DATA ANALYSIS </b> </font> <br><br>\n",
    " \n",
    "We **analyze** the distribution of the features _quality_ in the binary scenario. Namely, wines enjoying a score between 1 and 5 are **bad** whereas 6 and 10 as **good**.\n",
    "\n",
    "We conduct the analysis on the **training set**, avoiding therefore any modelling decision based on _unseen_ data (test set). In most cases, we assume that the distribution of the test set is similar to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 2.1. Target proportions  </font> <br>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.1]</b> Are the binary classes balanced? What are the proportions of data in each class? Briefly, justify your answer and add a visualization.\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3 : Target proportions\n",
    "   \n",
    "\n",
    "@pre:  Training dataframe  \n",
    "@post: proportion of each classes in this train set and a pie chart representing it.    \n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 2.2. Correlation matrix </font> <br>\n",
    "\n",
    "__Compute__ and __plot__ the correlation matrix. For the plot, you can use the function `imshow` or `matshow` from `matplotlib` or any other function that suits you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.2]</b> With the help of your plot and numerical results of cell 4, write your observations down.  <br\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°4 : CORRELATION MATRIX\n",
    "   \n",
    "@pre:  Training dataframe  \n",
    "@post: The correlation matrix between the features and its plot    \n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# compute and plot the correlation matrix\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> 2.3. Features Selection </font> <br>\n",
    "\n",
    "Establish a feature selection strategy [<sup>3</sup>](#fn3) and store the feature names in the corresponding variable. \n",
    "\n",
    "<span id=\"fn3\"> [3] N.B. E.g. selection of the $n$ first correlated features, setting a correlation threshold or any other rule.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.3]</b> Explain your strategy. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°5 : Features selection \n",
    "   \n",
    "@pre:  The correlation matrix  \n",
    "@post: The list 'feature_names' with the names of the selected features\n",
    "\"\"\"\n",
    "\n",
    "# TO DO\n",
    "feature_names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 2.4 Data scaling and binary target</font> <br> \n",
    "\n",
    "__Split__ your _training_ and _test_ sets into their respective features set  (X)  and a binary target variable (y) [<sup>4</sup>](#fn4). __Standardize__ the features sets. \n",
    "\n",
    "\n",
    "__Remark 1.__ The scaler object, used to standardize the training set, should also be the one used on the test set! Also, do no reinvent the wheel!  \n",
    "\n",
    "\n",
    "<span id=\"fn4\"> [4] Reminder - wines enjoying a quality score between 1 and 5 are **bad** (labelled 0) whereas 6 and 10 as **good** (labelled 1)</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 2.4]</b>  Why do we scale data? Justify properly, whether it is necessary or not for your feature set (X).\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°6 : Data scaling and binary target \n",
    "   \n",
    "@pre: train and test dataframes and the list 'feature_names' of columns to keep    \n",
    "@post:  X_train: numpy array, with (scaled) selected features, containing training data\n",
    "        X_test: numpy array, with (scaled) selected features, containing testing data. \n",
    "                The scaling should be done using the statistic of the train set.\n",
    "        y_train: numpy array of the target feature, containing training data. \n",
    "                It only contains 0s (for bad wines) and 1s (for good wines).\n",
    "        y_test: numpy array of the target feature, containing testing data. \n",
    "                It only contains 0s (for bad wines) and 1s (for good wines).\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Keep only the selected features\n",
    "X_train = data_train[feature_names]\n",
    "X_test = data_test[feature_names]\n",
    "\n",
    "# Binary target\n",
    "# TO MODIFY\n",
    "y_train = [0]*100\n",
    "y_test = [0]*10\n",
    "\n",
    "# Scaling\n",
    "# TO DO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999>  </font> <br>\n",
    "\n",
    "<br><font size=7 color=#009999> <b>PART 3 - MODEL SELECTION </b> </font> <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 3.1. PRECISION, RECALL AND F1 SCORE </font> <br>\n",
    "\n",
    "**Implement** the _precision, recall_ and _F-measure_ metrics based on the confusion matrix. Please follow the specifications in the provided template.  <br>\n",
    "\n",
    "**Reminder**\n",
    "\n",
    "$F_1$ is a performance metric allowing to obtain some trade-off between the precision and recall criterions. It can be computed as follows:\n",
    "$$F_1 = 2~\\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}.$$\n",
    "\n",
    "Please consult, [Wikipedia](https://en.wikipedia.org/wiki/F-score) for further information about the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°7 : IMPLEMENTATION OF PRECISION, RECALL AND F1 METRICS \n",
    "\n",
    "@pre:  /  \n",
    "@post: Follow the specifications to implement precision, recall and probas_to_F1 functions. \n",
    "\"\"\"\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "Converts a vector of real probability values to a binary 0 or 1 \n",
    "@pre: \n",
    "    - proba_vec: vector with real values representing each a probability to be in the class of good wines.\n",
    "    - threshold : a threshold probability (between 0 and 1) to determine if a wine is good ('1') \n",
    "@post:\n",
    "    - predicted_labels: binary prediction vector, with elements being 0 or 1.\n",
    "----------------------------------------------------------------------------------------- \"\"\"\n",
    "def pred_probas_to_pred_labels(proba_vec, threshold=0.5):\n",
    "    return np.where(proba_vec <= threshold, 0, 1)\n",
    "\n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "Based on the confusion matrix, computes the 'precision'\n",
    "@pre:\n",
    "    - cm : confusion_matrix of a binary classification\n",
    "@post:\n",
    "    - score: precision (or positive predictive value), associated with cm\n",
    "----------------------------------------------------------------------------------------- \"\"\"\n",
    "def precision(cm):\n",
    "    ppv = 1 # positive predictive value (or precision)\n",
    "    return ppv\n",
    "\n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "Based on the confusion matrix, computes the 'recall'\n",
    "@pre: \n",
    "    - cm : confusion_matrix of a binary classification  \n",
    "@post:\n",
    "    - r: recall (or true positive rate), associated with cm\n",
    "----------------------------------------------------------------------------------------- \"\"\"\n",
    "def recall(cm):\n",
    "    tpr = 1 # true positive rate (or recall)\n",
    "    return tpr\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" -----------------------------------------------------------------------------------------\n",
    "Evaluates the F1 score which is a harmonic mean of the precision and recall\n",
    "@pre: \n",
    "    - y_true: vectors of 0 and 1 representing the real class values\n",
    "    - y_pred: vectors of real values representing predicted probability of being in the class of good wines ('1')\n",
    "    - output:  'F1' means that the output should only be the F1 score. \n",
    "               'PRF1' means that the output is a tuple with (precision, recall, F1)\n",
    "               'F1' is the default value\n",
    "    - threshold: a threshold probability (between 0 and 1) to determine if a wine is good ('1')\n",
    "@post:\n",
    "    - F1_score: harmonic mean of the precision and recall\n",
    "    - If asked in argument, precision and recall can be added in the output: (precision, recall, F1)\n",
    "----------------------------------------------------------------------------------------- \"\"\"\n",
    "def probas_to_F1(y_true, y_pred, output=\"F1\",threshold=0.5):\n",
    "\n",
    "    y_pred = pred_probas_to_pred_labels(y_pred, threshold)\n",
    "    \n",
    "    # TO MODIFY\n",
    "    ppv = 0 # positive predictive value (or precision)\n",
    "    tpr = 0 # true positive rate (or recall)\n",
    "    F1_score = 0.0 \n",
    "\n",
    "    if output == \"PRF1\":\n",
    "        return (ppv, tpr, F1_score)\n",
    "\n",
    "    return F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 3.2. MODEL EVALUATION  </font> <br>\n",
    "\n",
    "**Implement** `evalParam`, which evaluates, using a __k-fold__ cross-validation, a list of `scikit-learn` models. Use your method `probas_to_F1` as score function. The function `evalParam` must be  **scalable**. Put differently, it must handle $n$ methods, and a variable list of their possible parameters configuration. \n",
    "\n",
    "\n",
    "In addition of the list of _models_ (methods) and their list of _hyperparameters_ (param), the function takes as arguments the _features set_ (X), _target variable_ (y) and _the number of folds_ (cv). \n",
    "\n",
    "It returns an array _score_ such that <br>\n",
    "\n",
    "$$score[i][j] = average F1 over the folds, using method _i_ with parameters configuration j.$$\n",
    "\n",
    "To help you, here is a pseudo code of K-fold for one method and one configuration of hyperparameters. \n",
    "\n",
    "<img src=\"Imgs/K-fold_pseudo-code.png\" width = \"650\">\n",
    " \n",
    "__Remark 1.__ You have to implement a K-fold cross-validation. You are only allowed to use `KFold.splits(dataset)` from `sklearn.model_selection` to generate the indices of your different folds. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.1]</b>\n",
    "    Explain the idea of K-fold cross-validation and why it is useful. How the choice of K (in the cross-validation) impacts the bias and the variance of the scores obtained on the different folds? Choose and justify the number of folds you consider in this project. \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°8 : Evaluates the methods using different parameters via a K-folds with cv folds\n",
    "\n",
    "@pre: \n",
    "    - methods: list of classifiers to analyze\n",
    "    - param: list of size len(methods) containing lists of parameters (in dictionary form) to evaluate.\n",
    "             In other words, param[i][j] is a dictionary of parmeters.\n",
    "             For example if index i is for KNN, we can have a parameter configuration (with index j) described as\n",
    "                 param[i][j] = {\"n_neigbors\":5, \"weights\": 'uniform'}; \n",
    "                 while param[i] is a list of such parameters dictionnaries for model i (here KNN)\n",
    "    - X: training dataset\n",
    "    - y: target vector for the corresponding entries of X\n",
    "    - cv: the number of folds to use in your cross-validation\n",
    "@post:\n",
    "    - score: list with same shape as param. score[i][j] = mean score over the folds, \n",
    "                                                         using method i with parameters param[i][j]\n",
    "------------------------------------------------------------------------------------------------ \"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evalParam(methods, param, X, y, cv):\n",
    "    score = [[0]]\n",
    "    # TO DO\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 3.3. MODEL SELECTION AND PARAMETERS TUNING </font> <br>\n",
    "\n",
    "__Run__ your function `evalParam` to evaluate the three following models : [_linear regression_](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), [_logistic regression_](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [_K nearest neighbors_](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). These models are already implemented in sklearn. <br>\n",
    "\n",
    "**Investigate** the effect of the following hyperparameters:\n",
    "- `n_neighbors` in KNN (try selected values between 1 and 100),\n",
    "- `weights` in KNN (try both values 'uniform' and 'distance')\n",
    "- `C` in logistic regression (try selected values between $10^{-3}$ and $10^3$)</li>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question you should ask yourself]</b> (Not graded) Prior to the run, discuss the fitness of each model to answer to our problem. \n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.2]</b> Explain your methodology of model evaluation. More precisely, explain which hyperparameters you tune and the values you test for each of them. Next, provide the best hyperparameters configuration for each of the three models as well as their CV F1 score.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°9 : Model selection - tuning the three methods\n",
    "   \n",
    "@pre:  evalParam function correctly implemented    \n",
    "@post: three models (knn, linear and logistic regression) initialized with tuned hyperparameters.\n",
    "        print the best hyperparameters found, as well as their CV F1 scores associated with these hyperparameters.\n",
    "------------------------------------------------------------------------------------------------ \"\"\"\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "linreg = LinearRegression()  # Linear Regression\n",
    "methods = [linreg]\n",
    "paramLR = [{\"normalize\": False}]  # no tuning to do for linreg but we put a default parameters here too fit in the input argument of evalParam\n",
    "param = [paramLR]\n",
    "cv = 1 #TO MODIFY according to your choice (see Question 3.1)\n",
    "\n",
    "# TO DO: add knn and logistic regression in methods and param arguments\n",
    "\n",
    "cv_scores = evalParam(methods, param, X_train, y_train, cv)\n",
    "\n",
    "# Finding the best parameters for each model\n",
    "# TO MODIFY for linear regression\n",
    "LRbest = 0  # to do\n",
    "print(\"Best param config for Linear Rgression is\", paramLR[LRbest], \"with score {0:.4f}\".format(cv_scores[0][LRbest]))\n",
    "linreg.set_params(**paramLR[LRbest])\n",
    "\n",
    "# TO DO for KNN and Logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.3]</b> Based on your answers to previous questions, select a final model that you will keep as classifier. Justify.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=#009999> <br> 3.4. Precision, Recall and thresholding </font> <br>\n",
    "\n",
    "In general, the classifying models compute first the probability for a point to belong to a certain class. Next, they applies a threshold to assign the final label (bad or good). By default, `scikit-learn` applies a threshold of 0.5 for KNN and logistic regression. You can use the function `predict_proba` to obtain the original probabilities. <br>\n",
    "For analyzing the impact of the threshold on the precision and recall of a model, we generally plot its precision-recall curve. Specific functions in sklearn help doing that plot (see [sklearn documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)). <br>\n",
    "Here we only ask you theoretical questions, so we don't expect any plot from you, but feel free to create some for your own reflection and to help you answering correctly.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.4]</b> What happens to the precision and recall (of any method) when the threshold tends to 0? And when it tends to 1? Explain and, if possible, establish a link with Question 2.1.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.5]</b> \n",
    "    Explain which precision/recall trade-off you prefer to have for the specific task asked in this hackathon: finding a good bottle of wines for your teaching assistants. How should you adjust the threshold of your model to bring it closer to the desired trade-off? Should it be above or below the default threshold value of 0.5? <br>\n",
    "    <b> Note: </b> In the next section, we will keep considering the default threshold value of 0.5.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br><font size=7 color=#009999> <b>PART 4 - MODEL TESTING </b> </font> <br><br>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 4.1]</b> Use the test set to estimate the precision, recall and F1 score of your final model and validate its performance on unseen data. <br> Observe if the scores are similar to the ones estimated with your cross-validation.\n",
    "        Are you satisfied by the performance of your classifier, in view of the task for which it will be used?\n",
    "\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°10 : MODEL TESTING\n",
    "   \n",
    "@pre:   clf is your selected classifier\n",
    "        X_test is the numpy array containing the test set (with your selected features)\n",
    "        y_test is the numpy array contaning your binary target vector\n",
    "@post:  print the F1, precision and recall on the test set.\n",
    "------------------------------------------------------------------------------------------------ \"\"\"\n",
    "\n",
    "y_pred = np.ones_like(y_test)  # to modif with your classifier prediction\n",
    "(prec, rec, F1) = probas_to_F1(y_test, y_pred, \"PRF1\") # metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lepl1109",
   "language": "python",
   "name": "lepl1109"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
