{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> \n",
    "<b>Hackathon 04 - Clustering: What is it all about?</b> </font> <br><br><br>\n",
    "\n",
    "<font size=5  color=#003366>\n",
    "Prof. D. Hainaut<br>\n",
    "Prof. L. Jacques<br>\n",
    "\n",
    "<br><br>\n",
    "Anne-Sophie Collin   (anne-sophie.collin@uclouvain.be)<br>\n",
    "Guillaume Van Dessel (guillaume.vandessel@uclouvain.be)<br>\n",
    "Loïc Van Hoorebeeck (loic.vanhoorebeeck@uclouvain.be)<br>\n",
    "Jérome Eertmans (jerome.eertmans@uclouvain.be)<br>\n",
    "Sébastien Colla (sebastien.colla@uclouvain.be)<br>\n",
    "Dani Manjah (dani.manjah@uclouvain.be)<br>\n",
    "<div style=\"text-align: right\"> Version 2021</div>\n",
    "\n",
    "<br><br>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=#009999> <b>GUIDELINES & DELIVERABLES</b> </font> <br>\n",
    "-  This assignment is due on the <b>18-12-2021 at 23h59</b>.\n",
    "-  Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups,...) must be clearly indicated!</b>\n",
    "-  This notebook (with the \"ipynb\" extension) file, the Python source file (\".py\"), and the report (PDF format) must be delivered on <b>Moodle</b>.\n",
    "- Only the PDF report and the python source file will be graded, both on their content and the quality of the text / figures. <br><br>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>[DELIVERABLE] Summary</b>  <br>\n",
    "After the reading of this document (and playing with the code!), we expect you to provide us with:\n",
    "<ol>\n",
    "   <li> a PDF file (written in LaTeX, see example on Moodle) that answers all the questions below. The report should contain high quality figures with named axes (we recommend saving plots with the <samp>.pdf</samp> extension);\n",
    "   <li> a Python file with your classifier implementation. Please follow the template that is provided and ensure it passes the so-called <i>sanity</i> tests;\n",
    "   <li> and this Jupyter Notebook (it will not be read, just checked for plagiarism).\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<font size=5 color=#009999> <b>CONTEXT & NOTEBOOK STRUCTURE</b> </font> <br>\n",
    "    \n",
    "The objective of this hackathon is threefold: (1) extract meaningful information from a dataset, (2) observe relationship(s) (if any) between features and eventual underlying groups (clusters), and (3) develop an unsupervised clustering tool. To this end, you will use a public dataset from Netflix (available on <b>Moodle</b>), alongside with a file describing the different ratings. Given a couple of features, you should be able to group movies (and TV shows) belonging to similar ratings.\n",
    "\n",
    "<img src=\"Imgs/image_article_binge_watching.png\" width = \"600\">\n",
    "\n",
    "Nowadays, the internet is crowded with millions of videos coming out every day. This is especially true on streaming platforms such as YouTube or Netflix. To keep their platform well organized, most of those companies rely on automated classification. For example, a video can be automatically classified to be rated R if it contains obscene scenes. <br> <br>\n",
    "Still, for most movies and TV-shows, this classification job is achieved manually... quite a <i>tedious</i> task... Your role aims at determining the implementation feasibility of an automated rating classification based on attributes such as *movie title* or the *actor/actress* that appear in it.\n",
    "\n",
    "This notebook is organized into three parts. Each of them assesses one fundamental step to solve our problem and provides one visualization tool to gain some understanding:\n",
    "* PART 1 - DATA PREPROCESSING\n",
    "   - 1.1 - Import the data\n",
    "   - 1.2 - Text data preprocessing\n",
    "   - 1.3 - Preliminary visualization tool \n",
    "    <br><br>\n",
    "* PART 2 - DATA VISUALIZATION \n",
    "   - 2.1 - PCA \n",
    "    <br><br>\n",
    "* PART 3 - IT'S TIME TO... CLUSTER!\n",
    "   - 3.1 - Customized K-Prototypes\n",
    "   - 3.2 - Results analysis \n",
    "   \n",
    "We filled this notebook with preliminary (trivial) code. This practice makes possible to run each cell, even the last ones, without throwing warnings. <b>Take advantage of this aspect to divide the work between all team members!</b> <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°0 : CHECK FOR NEW PACKAGES\n",
    "We added a few more package depedencies since S1, therefore we need to install them.\n",
    "Here, the process should be fully automated.\n",
    "\"\"\"\n",
    "\n",
    "# Few more packages we did not specify in previous requirements.txt\n",
    "# This should automatically install them.\n",
    "# If your installation is broken, please follow the same guidelines as in S1,\n",
    "# but with the new requirements.txt file\n",
    "try:\n",
    "    import wordcloud\n",
    "except ImportError as e:\n",
    "    !python -m pip install wordcloud\n",
    "try:\n",
    "    import seaborn\n",
    "except ImportError as e:\n",
    "    !python -m pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 1 - DATA PREPROCESSING</b> </font> <br><br>\n",
    "\n",
    "<font size=5 color=#009999> <b>1.1 - IMPORT THE DATA</b> <br>\n",
    "\"Netflix Shows\" dataset\n",
    "</font> <br> <br>\n",
    "\n",
    "In this __[dataset](https://www.kaggle.com/shivamb/netflix-shows)__, you are provided with movies and TV-shows, each having a series of features that will be useful all along this hackathon. If you are curious, the aforementionned link proposes several questions to be answered with this very dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°1 : IMPORT THE DATASET\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown as md\n",
    "from collections import Counter\n",
    "\n",
    "# See toolbox.py file\n",
    "import toolbox as tb\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 200)\n",
    "\n",
    "dataPath = \"Data/netflix_titles.csv\"\n",
    "descPath = \"Data/netflix_description.csv\"\n",
    "df = pd.read_csv(dataPath)\n",
    "desc = pd.read_csv(descPath, sep=\"\\t\")\n",
    "\n",
    "# Print first rows\n",
    "display(md(\"# Netflix titles\"))\n",
    "display(df.head())\n",
    "\n",
    "# Print content of ratings' description\n",
    "display(md(\"# Ratings' description\"))\n",
    "display(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>1.2 - FEATURES PREPROCESSING</b> <br>\n",
    "REMOVING UNNECESSARRY INFORMATION, CLEANING DATASET AND CREATING NEW FEATURES\n",
    "</font> <br> <br>\n",
    "\n",
    "When doing data sciences, the datasets you are using were most probably not made for your very application. Instead, they result from the collection of information throughout a certain period of time, and it is the data scientist's job to make a good use of those datasets.\n",
    "\n",
    "Today, the goal is to predict the `rating` category based on several other features. Therefore, you should be able to determine which of those are useful for your application.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.1] Removing unnecessary features </b>  <br>\n",
    "Can you already, a priori, detect that some features are useless?\n",
    "<ol>\n",
    "   <li> if yes, list those (useless) features;\n",
    "   <li> if not, then explain why it is better to wait.\n",
    "</ol>\n",
    "    Generally speaking, is it a good idea to remove a feature based on <i>a priori</i> knowledge, or it doesn't alter the final outcome?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark]</b><br>\n",
    "For the sake of simplicity, the test data will not contain any NaN. <br>However, we expect you to think about what you should do to handle NaNs.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°2 : REMOVING UNNECESSARY FEATURES\n",
    "If you answered yes in previous question, please remove those features here.\n",
    "\"\"\"\n",
    "\n",
    "#########################################################################################################\n",
    "# Start : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "features = [\n",
    "    # Fill the list here or leave empty\n",
    "]\n",
    "\n",
    "#########################################################################################################\n",
    "# End : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "df.drop(labels=features, axis=1, inplace=True, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark]</b><br>\n",
    "In most real-cases, the datasets you are going to work with will contain artifacts, such as typos or missing data, that you may want to remove before feeding the data into any algorithm. Here, Pandas treats missing data as NaNs (refering to Not a Number, even though it is used for every missing object, no only numbers).\n",
    "</div> \n",
    "\n",
    "Can you find a way to inspect your dataset and see the proportion of missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°3 : INFORMATION ABOUT TYPES AND NANs\n",
    "Use one (or more) of Pandas' builtin function to get information about data types\n",
    "and the number of missing (NaN) values for each feature.\n",
    "Do not reinvent the wheel: a good data-scientist is a lazy d-s.\n",
    "\"\"\"\n",
    "\n",
    "#########################################################################################################\n",
    "# Start : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "# End : Student version\n",
    "#########################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Each problem has its own solution</b> <br>\n",
    "There exists numerous ways to deal with missing information and we will discuss the two main approaches:\n",
    "<ol>\n",
    "   <li> You remove rows or columns that contain missing data;\n",
    "   <li> You replace NaNs with a another value. The latter can be a fixed value or computed to be the mean of all non-NaNs values. The topic of replacing missing data, also call imputation of missing values, is very broad and complex, and there is no global solution that applies everywhere. Maybe you can find one that works well here?\n",
    "</ol>\n",
    "</div> \n",
    "\n",
    "You **should** read more about how to imput missing value [here](https://scikit-learn.org/stable/modules/impute.html).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.2] Handling missing data </b>  <br>\n",
    "Given the dataset and the amount / type of missing information, what strategy do you propose to follow regarding missing data (NaNs) ?<br> You can choose one or many of the following:\n",
    "<ol>\n",
    "   <li> drop features (column) with missing information; \n",
    "   <li> drop samples (row) with missing information;\n",
    "   <li> replace missing information with interpolation / extrapolation / simple substitution / ...\n",
    "</ol>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°4 : HANDLING NANs\n",
    "Please fill the gaps according to what you have decided to do. Feel free to modify the code.\n",
    "\"\"\"\n",
    "\n",
    "#########################################################################################################\n",
    "# Start : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "drop_rows = [\n",
    "    # Fill the list here or leave empty\n",
    "]\n",
    "# set `drop_rows = None` to select all rows\n",
    "\n",
    "drop_cols = [\n",
    "    # Fill the list here or leave empty\n",
    "]\n",
    "# set `drop_cols = None` to select all columns\n",
    "\n",
    "# Fill special substitution here (e.g.: every NaN becomes an empty string \"\")\n",
    "\n",
    "#########################################################################################################\n",
    "# End : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "df.dropna(\n",
    "    axis=0, subset=drop_rows, inplace=True\n",
    ")  # Drop rows with NaN in any of the mentionned columns\n",
    "\n",
    "for col in drop_cols:\n",
    "    # For each mentionned column, drop it if it contains `any` NaN\n",
    "    if df[col].isnull().values.any():\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Last, we drop all rows with missing `rating` as it is useless to keep them in our model\n",
    "df.dropna(\n",
    "    axis=0, subset=[\"rating\"], inplace=True\n",
    ")  # Drop rows with NaN in any of the mentionned columns\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] New features extraction</b> <br>\n",
    "In the present case, some features in the dataset still need to be reworked in order to provide meaningful information. For example, the <samp>cast</samp> feature is a string listing multiple names. As it could be interesting to see if the presence of a given actor is linked to the rating (e.g.: some actors mostly play in comedy movies), you could extract all the actor names from this feature. Note that another aspect you could consider/take into account is the relative frequency of an actor apparition in the shows.\n",
    "</div>\n",
    "\n",
    "The following code will show you how to get the list of the actors playing in each of the shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°5 : MANIPULATING FEATURES\n",
    "Read the code carefully and try to understand it.\n",
    "\"\"\"\n",
    "\n",
    "feature = \"cast\"\n",
    "\n",
    "# In case we did not drop NaNs for `feature`, we do it now (or you can substitute NaNs with empty str)\n",
    "df.dropna(\n",
    "    axis=0, subset=[feature], inplace=True\n",
    ")  # Drop rows with NaN in any of the mentionned columns\n",
    "\n",
    "casting = []  # List of all actor names\n",
    "\n",
    "for movie_cast in df[feature].dropna():  # For all movies / TV-shows cast\n",
    "    casting.extend(  # We extend the list of actor names\n",
    "        movie_cast.split(\", \")  # with the list of names in current movie\n",
    "    )\n",
    "\n",
    "actors = set(casting)  # removes duplicates\n",
    "display(\n",
    "    md(\n",
    "        f\"\"\"# In total\n",
    "        There are {len(actors)} actors\n",
    "        An average of {len(actors) / df.shape[0]:.2} actors per movie / TV-shows\n",
    "        [{\", \".join(casting[:2])}, ..., {\", \".join(casting[-2:])}]\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "tb.word_cloud(casting)  # Plots a WordCloud of actor names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you don't have new features **yet**. You still need to somehow incorporate the information about actor names into the dataset in a more **intelligent** manner than it was before. Again, there can be multiple solutions, and we will propose you a very simple one.\n",
    "\n",
    "For every actor name in the list, we will create a new column (feature) in the dataset that will tell, for every row (sample), if the actor plays in that movie or not. This produces something similar to [One Hot Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). As there are many actors, many more than there are samples, we suggest you to only select the top-100 actors, by number of appearances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°6 : CREATING FEATURES\n",
    "Read the code carefully and try to understand it.\n",
    "Feel free to modify the number `n`.\n",
    "\"\"\"\n",
    "\n",
    "counts = Counter(casting)  # For each name, counts how many times it appears\n",
    "\n",
    "n = 100  # number of selected actors among the most appearing\n",
    "top_n = counts.most_common(n)  # Top n names by decreasing order of appearance\n",
    "\n",
    "display(pd.DataFrame(top_n, columns=[\"name\", \"count\"]).T)\n",
    "\n",
    "series = {}\n",
    "for name, count in top_n:\n",
    "    series[name] = (\n",
    "        df[feature].str.contains(name).astype(int)\n",
    "    )  # Does a cast contain the name of the actor?\n",
    "\n",
    "# We create a new dataset (we could use the same name though)\n",
    "casting_df = pd.concat(\n",
    "    series, axis=1\n",
    ")  # DataFrame where each row contains 0 or 1 depending on if the name appears in casting\n",
    "dataset = pd.concat([df, casting_df], axis=1)\n",
    "# We drop `cast` feature since we do not need it anymore\n",
    "dataset.drop(labels=[\"cast\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.3] All-zero representations? </b>  <br>\n",
    "For configurations <samp>n=100</samp> and <samp>n=None</samp>, count the number of sample whose encoded vector is the null vector [0, 0, ..., 0] (in <samp>casting_df</samp>). Why are there such vectors? How will they be treated by the clustering algorithm? <br>\n",
    "    \n",
    "<i>Note: <samp>n=None</samp> will select all the names, so it might take time and memory space!</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>[Bonus]</b>  <br>\n",
    "    In the case of the <samp>description</samp> feature, where each sample is a sentence, you could use more complex tools to extract information (e.g.: verbs, nouns, average sentence length, buzz words, etc.). Here, we can name a few:\n",
    "<ol>\n",
    "    <li> Regular Expression (with Python's package <a href=\"https://docs.python.org/3.8/library/re.html\", target=\"_blank\"><samp>re</samp></a>). We recommend <a href=\"https://regexone.com/\", target=\"_blank\">this blog</a> to understand how it works and <a href=\"https://regexr.com/\", target=\"_blank\">regexr.com</a> for testing your regular expressions;\n",
    "   <li> Natural Language Processing (NLP) with <a href=\"https://www.nltk.org/\", target=\"_blank\"><samp>nltk</samp></a>. Please read <a href=\"https://pub.towardsai.net/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0\", target=\"_blank\">this</a> for more information about NLP.\n",
    "</ol>\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 1.4] Feature extraction </b>  <br>\n",
    "Repeat the previous process for other features than <samp>cast</samp>. Some may require additional pre-processing. You might want to reuse the code above, or create a separate function to avoid code repetition. Do as you prefer!\n",
    "Then, answer the following questions:\n",
    "<ol>\n",
    "   <li> Which <b>new</b> features produce meaningful information for your classification?\n",
    "   <li> If it applies, explain the extra pre-processing you did to extract specific features.\n",
    "</ol>\n",
    "When answering, please provide quantitative (plots, measures, ...) information to support your claims.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°7 : OTHER NEW FEATURES\n",
    "Example of new feature creation. Please provide below your code that produces your own (new) features.\n",
    "\"\"\"\n",
    "\n",
    "#########################################################################################################\n",
    "# Start : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "try:\n",
    "    dataset[\"title-length\"] = dataset[\n",
    "        \"title\"\n",
    "    ].str.len()  # Is title length related to rating? Who knows\n",
    "except KeyError:\n",
    "    pass  # In case you remove the `title` feature ;-)\n",
    "\n",
    "#########################################################################################################\n",
    "# End : Student version\n",
    "#########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Before you can plot...</b> <br>\n",
    "Most graphical representations use numbers to determine how data should be plotted. Here, the dataset you are working with contains many non-numerical data types. Actually, all but the new features are strings (<samp>str</samp>). Sometimes, features are categorical, i.e., the set of possible values is finite (e.g.: <samp>type</samp> can only be <samp>\"Movie\"</samp> or <samp>\"TV Show\"</samp>). For those cases, it can be good practice to replace each category by a unique number (see <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder\"target=\"_blank\">Ordinal Encoder</a>). In other cases, features can take an infinite amount of value, such as for the <samp>description</samp>, and pre-processing must be done to take valuable information about it.\n",
    "</div>\n",
    "\n",
    "Here, we propose a very simple way of turning all the dataset features into numerical ones, or dropping them.\n",
    "Note that <b>you are not obliged</b> to encode your features into numerical ones. We just let you know how to deal with that task if this is something you consider doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°8 : OTHER NEW FEATURES\n",
    "Example of feature encoding and numeric types selection. \n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    dataset[\"type\"] = dataset[\"type\"].str.contains(\"Movie\").astype(int)  # Maybe\n",
    "    dataset[\"release_year\"] = dataset[\"release_year\"].astype(int)\n",
    "\n",
    "    reworked_durations = []\n",
    "    for duration in dataset[\"duration\"]:\n",
    "        num_time, scale = duration.split()\n",
    "        if scale == \"min\":\n",
    "            reworked_durations.append(int(num_time))\n",
    "        else:\n",
    "            reworked_durations.append(\n",
    "                int(num_time) * 180\n",
    "            )  # let us assume that a single season lasts for 3 hours :)\n",
    "    dataset[\"duration\"] = reworked_durations\n",
    "except KeyError:\n",
    "    pass  # For more robustness, we should do three try-except,\n",
    "    # one for each feature in [`type`, `release_year`, `duration`]\n",
    "\n",
    "numeric_dataset = dataset.select_dtypes([\"number\"])  # Selecting only number types\n",
    "\n",
    "numeric_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>[Bonus]</b>  <br>\n",
    "As you can probably see, many of the rating categories are quite similar. This leverages some crucial questions such as the importance of keeping all categories distinct or merging some of them together. The lower the number of classes, the better the classification?\n",
    "    \n",
    "Well you must find it by yourself! We have created a function that shows the distribution of ratings among the dataset (see below). You are allowed to modify the classes by remapping some of them into others, or by re-creating totally new ones.\n",
    "   \n",
    "<b>Nonetheless</b>, your new mapping must satisfy the following rules:\n",
    "<ol>\n",
    "    <li> there must be at least 4 different rating classes\n",
    "   <li> no class should contain more than 60% of the dataset\n",
    "</ol>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°9 : RATINGS FREQUENCY AMONG SAMPLE\n",
    "\"\"\"\n",
    "\n",
    "tb.get_ratings_freq(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°10 : RE-ENCODING TARGET FEATURE\n",
    "Feel free to comment or modify this section.\n",
    "\"\"\"\n",
    "\n",
    "#########################################################################################################\n",
    "# Start : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "# Exemple of (weird) mapping that satisfies requirements\n",
    "mapping = {\n",
    "    \"TV-MA\": \"ADULT\",\n",
    "    \"R\": \"ADULT\",\n",
    "    \"PG-13\": \"TEEN\",\n",
    "    \"TV-14\": \"TEEN\",\n",
    "    \"TV-PG\": \"GENERAL\",\n",
    "    \"NR\": \"NR\",\n",
    "    \"TV-G\": \"GENERAL\",\n",
    "    \"TV-Y\": \"YOUNG\",\n",
    "    \"TV-Y7\": \"YOUNG\",\n",
    "    \"PG\": \"GENERAL\",\n",
    "    \"G\": \"GENERAL\",\n",
    "    \"NC-17\": \"ADULT\",\n",
    "    \"TV-Y7-FV\": \"ADULT\",\n",
    "    \"UR\": \"NR\",\n",
    "}\n",
    "\n",
    "# Uncomment to update ratings\n",
    "tb.update_rating(dataset, mapping)\n",
    "\n",
    "#########################################################################################################\n",
    "# End : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "tb.get_ratings_freq(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font size=7 color=#009999> <b>PART 2 - DATA VISUALIZATION</b> </font> <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>2.1 - PCA</b> <br>\n",
    "REDUCE THE DIMENSIONALITY OF THE DATA IN ORDER TO OBSERVE IT\n",
    "</font> <br> <br>\n",
    "\n",
    "The high dimensionality of the dataset (number of columns) makes data visualization hard. In order to gain some (partial) information about our data distribution, we propose to perform a 3-dimensional visualization of them. For this purpose, we need to reduce dimensionality of our feature space while keeping as much information as possible about the data. \n",
    "\n",
    "PCA is often considered as the simplest and most fundamental technique used in dimensionality reduction. Remember that PCA is essentially the rotation of coordinate axes, chosen such that each successful axis captures or preserves as much variance as possible. If the algorithm returns a new system coordinates of the same dimension as the input, we can keep only the axis corresponding to the 3 largest singular values and project data on this coordinate system to perform the visualization.\n",
    "\n",
    "\n",
    "![PCAUrl](https://miro.medium.com/max/400/1*ZXhPoYQIn-Y8mxoUpz5Ayw.gif \"PCA\")\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 2.1] Clusters visualization </b>  <br>\n",
    "    Use the <samp>group_visualization</samp> function provided in the toolbox (<i>toolbox.py</i>) below to perform a 3-dimensional scatter plot of a <b>numerical</b> version of your data. \n",
    "Does the clustering problem seem to be difficult? Are the clusters separable? \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°11 : DATASET VISUALIZATION\n",
    "Apply PCA on the numerical part of your dataset. Keep the 2-3 most relevant PCA components to reduce\n",
    "the dimensionality of the inherent numerical data matrix. Then, plot the samples in the coordinate space of the 3 most \n",
    "relevant components with the group_visualization function. \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "target = dataset.rating.values\n",
    "dim = 3\n",
    "\n",
    "#########################################################################################################\n",
    "# Start : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "shape = (target.size, dim)\n",
    "\n",
    "# Change this!!\n",
    "X = np.random.random(shape)\n",
    "\n",
    "#########################################################################################################\n",
    "# End : Student version\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "tb.group_visualization(target, X, less_points=True, save_fig=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=7 color=#009999> <b>PART 3 - IT'S TIME TO ... CLUSTER!</b> </font> <br><br>\n",
    "\n",
    "<br>\n",
    "<font size=5 color=#009999> <b>3.1 - Clustering: definition </b> <br>\n",
    "THE ABC OF CLUSTERING\n",
    "</font> <br> <br>\n",
    "\n",
    "Clustering can be defined as the task of *grouping* objects from a set $S$ (here, each row/observation is an object) in such a way that objects assigned to the same group (called cluster) are more **similar** (or less **distant**) with respect to each other (in some sense) than to those assigned to the other groups. Usually, we would like to divide our objects into $K$ groups.\n",
    "\n",
    "As such, clustering reduces to finding, among all $K$-partitions possible of $S$, the partition $\\mathcal{P}$ that minimizes some error criterion $f(\\mathcal{P})$. Each object will be assigned a cluster, $C_i$, and each cluster will have its centroid $c_i$ the distance between **any object** in $C_i$ to centroid $c_i$ is **always smaller** that the distance to any other centroid. In other words, each object is assigned to the cluster whose centroid is the closest.\n",
    "\n",
    "\n",
    "A mathematical formulation of the problem could be the following, $$ \\boxed{\\min_{(C_1,\\dots,C_K) \\,\\in\\, \\mathcal{P}}\\,f(C_1,\\dots,C_K) = \\sum_{i = 1}^{K}\\,\\sum_{x \\in C_i}\\,\\Delta(x,c_i)}$$\n",
    "\n",
    "where $\\Delta(x,c_i)$ denotes the distance between object $x$ and centroid $c_i$.\n",
    "\n",
    "<br>\n",
    "<font size=5 color=#009999> <b>3.2 - Clustering: example </b> <br>\n",
    "EXAMPLE OF SEPARATING OBJECTS INTO 10 CLUSTERS\n",
    "</font> <br> <br>\n",
    "\n",
    "**First**, let us imagine the following 2D dataset.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-data.svg\" width = \"250\">\n",
    "\n",
    "**Then**, a 10-partition is defined by the position of the centroids, one for each cluster. Below, you can observe four examples of (random) centroids localizations (stars).\n",
    "\n",
    "<img src=\"Imgs/10-partitions-chose-centroids.svg\" width = \"1000\">\n",
    "\n",
    "**Next**, the regions are colored based on their closest centroid. Here, we take the distance to be the Euclidean distance.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-centroids.svg\" width = \"1000\">\n",
    "\n",
    "**Finally**, data points (objects) are colored based in the region they are in.\n",
    "\n",
    "<img src=\"Imgs/10-partitions-clusters.svg\" width = \"1000\">\n",
    "\n",
    "<br>\n",
    "<font size=5 color=#009999> <b>3.3 - Customized clustering with K-Prototypes </b> <br>\n",
    "DEFINE YOUR OWN DISTANCE FUNCTION AND IMPLEMENT A CLUSTERING ALGORITHM\n",
    "</font> <br> <br>\n",
    "\n",
    "In the example above, the distance was the well-known Euclidean distance. However, a variety of metrics can be used and what you will use depends on the problem your are working on. [scikit-learn](https://scikit-learn.org/stable/modules/classes.html#pairwise-metrics) offers many possibilities, but we encourage you to implement yours!\n",
    "\n",
    "For your $\\Delta$ function to be valid, it should admit the following properties:\n",
    "\n",
    "1. $\\Delta(a,b) = 0 \\Longleftrightarrow a = b$\n",
    "2. $\\Delta(a,b) = \\Delta(b,a)$\n",
    "3. $\\Delta(a,b) \\in [0,\\infty[$\n",
    "\n",
    "In many cases, the centroids localization is not optimal. Therefore, after clustering, objects in each cluster can choose a new centroid for their cluster.\n",
    "\n",
    "The computation if the new centroid is performed with,\n",
    "$$\\boxed{ c_i \\gets \\arg \\min_{c}\\, \\frac{1}{|C_i|} \\sum_{x \\in C_i}\n",
    "\\,\\Delta^2(x,c)}$$\n",
    "\n",
    "where $c$ can be any vector (object) and $|C_i|$ refers to the number of objects in the clusters.\n",
    "\n",
    "Typically, if one uses the Euclidean distance $\\Delta(a,b) = ||a-b||_2$, then $c_i$ simply stands as the **mean** vector of its inherent cluster, \n",
    "\n",
    "$$ c_i = \\frac{1}{|C_i|} \\sum_{x \\in C_i} \\,x $$\n",
    "\n",
    "A $K$-Prototypes methods is therefore an algorithm that divides the observation set into $K$ clusters, by applying an arbitrary distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Centroids are not observations </b>  <br>\n",
    "    <b>Centroids</b> do not necessarily belong to the set of observations, $S$. They represent as much as possible their cluster as being, in <i>average</i>, the most similar to the objects within the cluster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our $K$-Prototypes method it to return $K$ centroids from a suitable $K$ partition of your dataset and assign each object to its most **similar** centroid.\n",
    "\n",
    "For this purpose, the algorithm iterates between two steps until a stopping criteria is reached (i.e., data points are assigned to the same cluster, maximum number of iterations, ...). Let $(C_{1,t},\\dots,C_{K,t})$ and $(c_{1,t},\\dots,c_{K,t})$ respectively represent the current **clusters** and **centroids** after $t$ iterations. \n",
    "\n",
    "Then, one iteration of the algorithm can divided into two parts:\n",
    "1. **Data assignment step:** each data point $x \\in S$ is assigned to its closest centroid.    \n",
    "2. **Centroid update step:** new **centroids** $c_{i,t+1}$ are computed, and if possible such that the sum of squared distances in minimized.\n",
    "\n",
    "As detailed aboved, steps are repeated until convergence is observe or a maximum number of iterations has been performed.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Empty clusters </b>  <br>\n",
    "    At this step, what could happend one cluster becomes *depleted* (i.e., empty)? Find a way to handle properly such a situation.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Convergence </b>  <br>\n",
    "    K-Prototypes algorithm is guaranteed to converge to a result. However, the result may be a local optimum (i.e. not necessarily the best possible outcome), meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 3.1] Convergence of your centroids </b>  <br>\n",
    "    How can you determine if your centroids are converging or not?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Example</b>  <br>\n",
    "K-Means is K-Prototypes in the Euclidean setting described above. One equivalently tries to minimize the error criterion <i>Sum of Squared Squares</i> : $$f(\\mathcal{P}) = \\sum_{i=1}^{K}\\,\\sum_{x \\in C_i} ||a-b||_2^2$$\n",
    "</div> \n",
    "\n",
    "![KMeansUrl](https://dashee87.github.io/images/kmeans.gif \"KMeans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>[Question 3.2] Implement a meaningful distance function $\\Delta$. </b>  <br>\n",
    "    Based on the data types of your final dataset (after feature selection/extraction), define an appropriate $\\Delta$ function to be used within your <br>$K$-Prototypes. Discuss your decision and the potential impact it can have on determining the optimal centroid location.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] Distance on categorical data </b>  <br>\n",
    "    Until here, we presented results on numerical data. However, you can also define distance function that work with categorical data (e.g., the movie type), something that could be applied here. You can find examples <a href=\"https://towardsdatascience.com/the-k-prototype-as-clustering-algorithm-for-mixed-data-type-categorical-and-numerical-fe7c50538ebb\">here</a>.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°12 : DISTANCE METRIC\n",
    "Based on the templates below, define a custom metric for your needs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def delta_euclidian(a, b, weights=None):\n",
    "    # Same as `scipy.spatial.distance.euclidean`\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(a))\n",
    "    return sum(weights * (a - b) ** 2)\n",
    "\n",
    "\n",
    "def delta_mode(a, b, weights=None):\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(a))\n",
    "    return sum(weights * (a != b))\n",
    "\n",
    "\n",
    "# Your metric\n",
    "def distance_metric(a, b, **kwargs):\n",
    "    \"\"\"\n",
    "    A pairwise distance between vectors a and b.\n",
    "\n",
    "    :param a: a vector\n",
    "    :param b: a vector\n",
    "    :param kwargs: any keyword arguments you would like to add..\n",
    "    \"\"\"\n",
    "    #########################################################################################################\n",
    "    # Start : Student version\n",
    "    #########################################################################################################\n",
    "\n",
    "    return delta_euclidian(a, b, weights=kwargs.get(\"weights\"))\n",
    "\n",
    "    #########################################################################################################\n",
    "    # End : Student version\n",
    "    #########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°13 : K-PROTOTYPES IMPLEMENTATION\n",
    "Implement here your K-Prototypes algorithm.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def KPrototypes(\n",
    "    X,\n",
    "    k=5,\n",
    "    n_max=100,\n",
    "    metric=distance_metric,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a K-Prototypes algorithm on input data.\n",
    "\n",
    "    :param X: m-by-n observation matrix or DataFrame\n",
    "    :param k: number of clusters\n",
    "    :param n_max: maximum number of iterations\n",
    "    :param metric: the distance metric to use\n",
    "    :param kwargs: optional arguments to be passed to `metric`\n",
    "    :return: the cluster labels and the centroids\n",
    "    \"\"\"\n",
    "\n",
    "    #########################################################################################################\n",
    "    # Start : Student version\n",
    "    #########################################################################################################\n",
    "\n",
    "    # Change this\n",
    "    centroids = np.random.rand(k, X.shape[1])\n",
    "    cluster_labels = np.zeros(X.shape[0], dtype=X.dtype)\n",
    "\n",
    "    # Do some iterations...\n",
    "\n",
    "    ...\n",
    "\n",
    "    #########################################################################################################\n",
    "    # End : Student version\n",
    "    #########################################################################################################\n",
    "\n",
    "\n",
    "    # Return the results\n",
    "\n",
    "    return cluster_labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b> Apply the K-Prototypes clustering algorithm on your dataset. </b>  <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>[Remark] K-Prototypes can be slow </b>  <br>\n",
    "    Depending on how you implement $K$-Prototypes, or even your $\\Delta$ function, the processing of your dataset can take from a few seconds to minutes. For debugging purpose, we encourage you to downsample your dataset to reduce computation time. To this end, we provide you with the necessary code below.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°14 : DOWNSAMPLING\n",
    "Optionally, downsample your dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Set to True if you would like to downsample\n",
    "downsample = False\n",
    "\n",
    "if downsample:\n",
    "\n",
    "    fraction = 0.3  # % of dataset you will consider for the rest of your analysis\n",
    "    assert 0 < fraction <= 1\n",
    "\n",
    "    uniques = np.unique(target)\n",
    "\n",
    "    n = len(target)\n",
    "    size = int(n * fraction)\n",
    "\n",
    "    selected = np.zeros_like(target, dtype=bool)\n",
    "\n",
    "    for unique in uniques:\n",
    "        indices = np.argwhere(target == unique).flatten()\n",
    "        size = max(1, int(len(indices) * fraction))\n",
    "        indices = np.random.choice(indices, size=size, replace=False)\n",
    "        selected[indices] = True\n",
    "\n",
    "    target = target[selected]\n",
    "    X = X[selected, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°15 : K-PROTOTYPES EXECUTION\n",
    "Simply run this cell.\n",
    "\"\"\"\n",
    "\n",
    "k = min(5, len(np.unique(target)))\n",
    "\n",
    "use_KPrototype = False  # Set to True when ready to test your implementation\n",
    "\n",
    "if use_KPrototype:\n",
    "    cluster_labels, centroids = KPrototypes(X, k=k)\n",
    "else:\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=5 color=#009999> <b>3.2 - RESULTS ANALYSIS </b> <br>\n",
    "OBSERVE AND COMPARE\n",
    "</font> <br> <br>\n",
    "\n",
    "In this section, we adress the difficult task of evaluating the performance of the clustering algorithm. We suggest you to work in 3 steps : \n",
    "<br> \n",
    "1. <b>Look at the result.</b> Before any quantitative study, you should look at your clustering in the low dimensional space. It should help you to get some insight. We already made this step in part 2. \n",
    "<br><br>    \n",
    "2. <b>Perform a silhouette analysis.</b> One way to assess the quality of the data partitionning is the use of the silhouette analysis. Each silhouette shows which object lies well within their cluster, and which one is merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The coefficient varies between -1 and 1. A value close to 1 implies that the instance is close to its cluster is a part of the right cluster. Whereas, a value close to -1 means that the value is assigned to the wrong cluster.\n",
    "<br><br>\n",
    "3. <b>Compute the confusion matrices.</b> In the context where some labeled data are available for assigning clusters to known labels, we need to find which cluster corresponds to which label. For this purpose we have to find the label permutation that leads to the best [accuracy](https://smorbieu.gitlab.io/accuracy-from-classification-to-clustering-evaluation/). As there are $K!$ permutations, testing all of them is not a viable solution. Therefore, we encourage you to manually test some permutations and, then, derive a method to find a good permutation (without trying all of them!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°16 : PAIRWISE DISTANCE.\n",
    "Warning: if you use your own metric, it might take much longer to compute.\n",
    "\"\"\"\n",
    "distance_matrix = tb.pairwise_distance(X, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°17 : SILHOUETTE ANALYSIS\n",
    "Analyse how your clustering algorithm performs.\n",
    "\"\"\"\n",
    "\n",
    "tb.silhouette_visualization(cluster_labels, distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL N°18 : CONFUSION MATRIX\n",
    "Observe the impact of your cluster mapping on the confusion matrix, and try to determine to best mapping possible.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cluster_mapping = {0: \"TEEN\", 1: \"ADULT\", 2: \"NR\", 3: \"YOUNG\", 4: \"GENERAL\"}\n",
    "tb.confusion_matrix_visualization(target, cluster_labels, cluster_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.3] Silhouette plots</b>  <br> \n",
    "    Analyze results of your clustering method thanks to <i>Silhouette</i> graphs as well as <i>Confusion</i> matrices. <br>\n",
    "    Try to be quite exhaustive and present/comment your findings to prove us you have taught about great features :)\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>[Question 3.4] Mapping clusters to ratings</b>  <br> \n",
    "    Given all the possible permutations, how would do to determine a good cluster mapping, i.e., a permutation with a high accuracy, without testing all possible mappings?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should be able to create a complete pipeline, from start to end, that can train a model on a data, and output some prediction for a given input vector. **However**, you do not have to implement such thing here. This will be seen during, e.g., the Machine Learning [LELEC2870](https://uclouvain.be/en-cours-2021-lelec2870) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lepl1109",
   "language": "python",
   "name": "lepl1109"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
